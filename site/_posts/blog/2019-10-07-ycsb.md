---
layout: post
title: "YCSB Benchmark with Crail on DRAM, Flash and Optane over RDMA and NVMe-over-Fabrics"
author: Patrick Stuedi and Jonas Pfefferle
category: blog
comments: true
---

<div style="text-align: justify"> 
<p>
Recently, suppport for Crail has been added to the <a href="https://github.com/brianfrankcooper/YCSB">YCSB</a> benchmark suite. In this blog we describe how to run the benchmark and briefly show some performance comparisons between Crail and other key-value stores running on DRAM, Flash and Optane such as <a href="https://www.aerospike.com">Aerospike</a> or <a href="https://ramcloud.atlassian.net/wiki/spaces/RAM/overview">RAMCloud</a>. 
</p>
</div>

### The Crail Key-Value Storage Namespace

<div style="text-align: justify"> 
<p>
Remember that Crail exports a hierarchical storage namespace where individual nodes in the storage hierarchy can have different types. Supported node types are Directory (Dir), File, Table, KeyValue (KV) and Bag. Each node type has slightly different properties and operations users can execute on them, but also restricts the possible node types of its child nodes. For instance, directories offer efficient enumeration of all of its children, but restricts children to be either of type Directory of File. Table nodes allow users to insert or retrieve KV nodes using a PUT/GET API, but restricts the children to be of type KV. All nodes, independent of their type, are identified using path names encoding the location in the storage hierarchy, similar to files and directories in a file system. All nodes further consist of metadata, managed by one of Crail's metadata servers, and an arbitrary large data sets, distributed across Crail's datanodes. For a more detailed description of Crail's node types please consider our recent <a href="https://www.usenix.org/conference/atc19/presentation/stuedi">USENIX ATC'19 </a> paper. 
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://127.0.0.1:4000/img/blog/ycsb/storage_namespace.svg" width="400"></div>
<br> 
<br>

<div style="text-align: justify"> 
<p>
In this blog we focus on Crail's KeyValue API available to users through the Table and KV node types. Creating a new table and inserting a key-value pair into it can be done as follows.
 </p>
</div>

```
CrailStore crail = CrailStore.newInstance();
CrailTable table = fs.create("/tab1", CrailNodeType.TABLE, ..., ...).get();
CrailKeyValue kv = fs.create("/tab1/key1", CrailNodeType.KEYVALUE, ..., ...).get();
``` 

<div style="text-align: justify"> 
<p>
Here the table's name is "/tab1" and the key of the key-value pair is "key1". Unlike in a traditional key-value store where the value of a key is defined when inserting the key, in Crail the value of the key consists of an arbitrary size append-only data set, that is, a user may set the value of a key by appending data to it as follows. 
 </p>
</div>

```
CrailOutputStream stream = kv.getOutputStream();
CrailBuffer buf = CrailBuffer.wrap("data".bytes());
stream.append(buf);
``` 
<div style="text-align: justify"> 
<p>
Lookup and reading of a key-value pair is done in a similar fashion. 
 </p>
</div>

```
CrailStore crail = CrailStore.newInstance();
CrailKeyValue kv = fs.lookup("/tab1/key1").get().asKeyValue();
CrailInputStream stream = kv.getInputStream();
CrailBuffer buf = crail.createBuffer();
stream.read(buf);
``` 

<div style="text-align: justify"> 
<p>
Note that multiple clients may concurrently try to create a key-value pair with the same name in the same table. In that case Crail provides last-put-wins semantics where the most recently created key-value pair prevails. Clients currently reading or writing a stale key-value pair will be notified about the staleness of their object upon the next data access. 
 </p>
</div>
 
### Running YCSB with Crail

<div style="text-align: justify"> 
<p>
The <a href="https://github.com/brianfrankcooper/YCSB">YCSB</a> benchmark is a popular benchmark to measure the performance of key-value stores in terms of PUT/GET latency and throughput for different workloads. We recently contributed support for Crail to the benchmark are we are excited that the Crail binding got accepted and integrated into the benchmark last June. With Crail, users can now run NoSQL workloads over Crail's RDMA and NVMe-over-fabrics storage tiers. 
</p> 
</div>  

<div style="text-align: justify"> 
<p>
In order to run the benchmark simply clone the YCSB repository and build the Crail binding as follows. 
</p> 
</div>   

```
crail@clustermaster:~$ git clone http://github.com/brianfrankcooper/YCSB.git
crail@clustermaster:~$ cd YCSB
crail@clustermaster:~$ mvn -pl com.yahoo.ycsb:crail-binding -am clean package
```   

<div style="text-align: justify"> 
<p>
You need to have a Crail deployment up and running to run the YCSB benchmark. Follow the <a href="https://incubator-crail.readthedocs.io/en/latest">Crail documentation</a> if you need help with configuring and deploying Crail. Once Crail is up and accessible, data can be generated and loaded into Crail as follows. 
</p> 
</div>  

```
crail@clustermaster:~$ ./bin/ycsb load crail -s -P workloads/workloada -P large.dat -p crail.enumeratekeys=true >outputLoad.txt
```  


<div style="text-align: justify"> 
<p>
In this case we are running workload A which is an update heavy workload. Different workloads for different read/update ratios can be specified using the -P switch. The size of the data -- or more precisely -- the number of records to be written, can be defined via the YSCB property "recordcount". You can define arbitrary number of YSCB properties in a file (e.g., "large.dat") and pass the name of the file to the YSCB benchmark when loading the data. Note the Crail YCSB binding will pick up all the Crail configuration parameters defined in "$CRAIL_HOME/crail-site.conf". In the above example we further use "crail.enumeratekeys=true" which is a parameter specific to the Crail YCSB binding that enable enumeration of Crail tables. Enumeration support is convenient as it allows browsing of tables using the Crail command line tools. During actual performance measurements, however, it is recommended to run off enumeration support which is faster. 
</p> 
</div> 

<div style="text-align: justify"> 
<p>
So far we have just loaded the data. Let's now run the actual benchmark which consists of a series of read and update operations. 
</p> 
</div>  

```
crail@clustermaster:~$ ./bin/ycsb run crail -s -P workloads/workloada
```  

### YCSB Benchmark Performance for DRAM & Intel Optane

<div style="text-align: justify"> 
<p>
We ran workload B of the YSCB benchmark using the Crail binding. Workload B has 95% read and 5% update operations and the records are selected with a Zipfian distribution. The two figures below show the update latencies for two different sizes of key-value pairs, 1K (10 fields of 100 bytes per KV pair) and 100K (10 fields of 10KB per KV pair). We show the cumulative distribution function for both Crail's DRAM/RDMA storage tier and the NVMf storage tier running on Intel Optane SSDs. As a reference we also report the update performance for RAMCloud and Aerospike on the same hardware, that is, RAMCLoud on RDMA and Aerospike on Optane. All Crail experiments run in a single namenode single datanode configuration with the YSCB benchmark executing remote to both namenode and datanode.
</p>
<p>
Looking at small KV pairs first (left figure above), we can see that Crail's DRAM storage tier delivers update latencies that are about 5-10us higher than those of RAMCloud for a large fraction of updates. At the same time, Crail's Optane storage tier delivers update latencies that are substantially better than those of Aerospike on Optane, namely, 50us for Crail vs 100us for Aerospike for a large fraction of updates.  
</p>
</div> 

<br>
<div style="text-align:center"><img src ="http://127.0.0.1:4000/img/blog/ycsb/ycsb_get.svg" width="700"></div>
<br> 
<br>

<div style="text-align: justify"> 
<p>
The reason Crail trails RAMCloud for small KV pairs is because Crail's internal message flow for implementing a PUT operation is more complex than the message flow of a PUT in most other distributed key-value stores. This is illustrated by the two figures below. A PUT operation in Crail (left figure below) essentially requires two metadata operations to create and close the key-value obejct, and one data operation to write the actual "value". Involving the metadata server adds flexibility becauuse it allows Crail to dynamically choose the best storage server or the best storage tier for the given value, but it also adds two extra roundtrips compared to a PUT in a traditional key-value store (right figure below). In Crail we have designed the RPC subsystem between clients and metadata servers to be extremely light and low-latency, which in turn allows us to favor flexibility over absolut lowest performance during PUT/GET operations. 
 

As shown forthis default setup, the largest number of read operations ob-serve a latency of 14μs(95% and 99% percentile are at 37and 84μs) and 26μs(95% and 99% percentile are at 47 and81μs) for DRAM and Optane respectively. Crail on Optanehas an average latency of 38μsand thus is only 15μsslowerthan Crail on DRAM. On the other hand, Aerospike with Op-tane delivers an average latency of 108.7μs, which is 2.84×worse than the average Crail latency (38.03μs). ComparingCrail’s DRAM performance to RAMCloud shows that RAM-Cloud is slightly faster than Crail. However, as we move tolarger values of 10 fields of 10KB each (100KB per KV pair)in Figure 8 (bottom) Crail is almost2.6−4.8×better thanAerospike and RAMCloud, respectively. 
</p>
</div>  

<br>
<div style="text-align:center"><img src ="http://127.0.0.1:4000/img/blog/ycsb/crail_put_anatomy.svg" width="550"></div>
<br> 
<br>

<div style="text-align: justify"> 
<p>
Ideally, we would want individual storage tiers to be elastic in a way that storage capacities can be adjutsed dynamically (and automatically) based on the load. Currently, Crail does not provide elastic storage tiers (adding storage servers on the fly is always possible, but not removing). A recent research project has been exploring how to build elastic storage in the context of serverless computing and in the future we might integrate some of these ideas into Crail as well. Have a look at the <a href="https://www.usenix.org/system/files/osdi18-klimovic.pdf">Pocket OSDI'18</a> paper for more details or check out the system at <a href="https://github.com/stanford-mast/pocket">https://github.com/stanford-mast/pocket</a>. 
</p>
</div>  

<br>
<div style="text-align:center"><img src ="http://127.0.0.1:4000/img/blog/ycsb/ycsb_put.svg" width="700"></div>
<br> 
<br>

### Pushing the throughput limits

<div style="text-align: justify"> 
<p>
Ideally, we would want individual storage tiers to be elastic in a way that storage capacities can be adjutsed dynamically (and automatically) based on the load. Currently, Crail does not provide elastic storage tiers (adding storage servers on the fly is always possible, but not removing). A recent research project has been exploring how to build elastic storage in the context of serverless computing and in the future we might integrate some of these ideas into Crail as well. Have a look at the <a href="https://www.usenix.org/system/files/osdi18-klimovic.pdf">Pocket OSDI'18</a> paper for more details or check out the system at <a href="https://github.com/stanford-mast/pocket">https://github.com/stanford-mast/pocket</a>. 
</p>
</div> 

<br>
<div style="text-align:center"><img src ="http://127.0.0.1:4000/img/blog/ycsb/iops_qd1.svg" width="650"></div>
<br> 
<br>

<div style="text-align: justify"> 
<p>
Ideally, we would want individual storage tiers to be elastic in a way that storage capacities can be adjutsed dynamically (and automatically) based on the load. Currently, Crail does not provide elastic storage tiers (adding storage servers on the fly is always possible, but not removing). A recent research project has been exploring how to build elastic storage in the context of serverless computing and in the future we might integrate some of these ideas into Crail as well. Have a look at the <a href="https://www.usenix.org/system/files/osdi18-klimovic.pdf">Pocket OSDI'18</a> paper for more details or check out the system at <a href="https://github.com/stanford-mast/pocket">https://github.com/stanford-mast/pocket</a>. 
</p>
</div> 

<br>
<div style="text-align:center"><img src ="http://127.0.0.1:4000/img/blog/ycsb/iops_qd4.svg" width="650"></div>
<br> 
<br>

### Summary

<div style="text-align: justify"> 
<p>
In this blog we discussed various configuration options in Crail for deploying tiered disaggrated storage. Crail allows mixing traditional non-disaggregated storage with disaggregated storage in a single storage namespace and is thereby able to seamlessly absorb peak storage demands while offering excellent performance during regular operation. Storage classes and location classes in Crail further provide fine-grained control over how storage resources are provisoned and allocated. In the future, we are considering to make resource provisioning in Crail dynamic and automatic, similar to <a href="https://www.usenix.org/system/files/osdi18-klimovic.pdf">Pocket</a>. 
 </p>
 </div>

 
